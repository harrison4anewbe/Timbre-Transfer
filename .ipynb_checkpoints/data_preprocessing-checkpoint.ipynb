{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize paired '.wav' audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SAMPLING_RATE=44100\n",
    "NSYNTH_SAMPLE_RATE=16000\n",
    "NSYNTH_VELOCITIES=[25, 50, 100, 127]\n",
    "hop_length = 512\n",
    "bins_per_octave = 16 * 2\n",
    "amin=1/(2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruments: \t 2 ['keyboard', 'string']\n",
      "MIDI files: \t 2\n"
     ]
    }
   ],
   "source": [
    "args={'nsynth_path':'.//nsynth-train//audio'\n",
    "    ,'midi_path':'.//archive'\n",
    "    ,'audios_path':'.//data//audios'\n",
    "    ,'playback_speed':1\n",
    "    ,'duration_rate':4\n",
    "    ,'transpose':0}\n",
    "\n",
    "\n",
    "instruments = [\n",
    "    {'name': 'keyboard', 'source_type': 'acoustic', 'preset': 0},\n",
    "    {'name': 'string', 'source_type': 'acoustic', 'preset': 0}\n",
    "]\n",
    "\n",
    "midifiles=[]\n",
    "music_types = os.listdir(args['midi_path'])\n",
    "for Type in music_types:\n",
    "    for music in os.listdir(args['midi_path']+'//'+Type):\n",
    "        midifiles.append(args['midi_path']+'//'+Type+'//'+music) # Get all mdi files path\n",
    "        \n",
    "print()\n",
    "print(\"Instruments: \\t\", len(instruments), [instrument['name'] for instrument in instruments])\n",
    "print(\"MIDI files: \\t\", len(midifiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteSynthesizer():\n",
    "    def __init__(self, dataset_path, sr=44100, transpose=0, leg_stac=.9, velocities=np.arange(0,128), preset=0, preload=True):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.sr = sr\n",
    "        self.transpose = transpose\n",
    "        self.leg_stac = leg_stac\n",
    "        self.velocities = velocities\n",
    "        self.preset = preset\n",
    "\n",
    "        self.preload = preload\n",
    "\n",
    "    def _quantize(self, value, quantized_values):\n",
    "        diff = np.array([np.abs(q - value) for q in quantized_values])\n",
    "        return quantized_values[diff.argmin()]\n",
    "\n",
    "    def _get_note_name(self, note, velocity, instrument, source_type, preset=None):\n",
    "        if preset is not None:\n",
    "            preset = preset\n",
    "        else:\n",
    "            preset = self.preset\n",
    "        return instrument+'_'+source_type+'_'+str(preset).zfill(3)+'-'+str(note).zfill(3)+'-'+str(velocity).zfill(3)+'.wav'\n",
    "\n",
    "    def preload_notes(self, instrument, source_type, preset=None):\n",
    "        preset = preset if(preset is not None) else self.preset\n",
    "        print(\"Preloading notes for \" + instrument + \"_\" + source_type + \"_\" + str(preset).zfill(3))\n",
    "        self.notes = {}\n",
    "        for n in range(22, 108):\n",
    "            for v in self.velocities:\n",
    "                note_name = self._get_note_name(n, v, instrument, source_type, preset)\n",
    "                try:\n",
    "                    audio, sr = librosa.load(os.path.join(self.dataset_path, note_name), sr=self.sr) # get audio (return audio and sampling rate)\n",
    "                except:\n",
    "                    audio = None\n",
    "                self.notes[note_name] = audio\n",
    "        print(\"Notes loaded\")\n",
    "\n",
    "    def _render_note(self, note_filename, duration, velocity):\n",
    "        try:\n",
    "            if(self.preload):\n",
    "                note = self.notes[note_filename]\n",
    "            else:\n",
    "                note, _ = librosa.load(note_filename) # load synth music data\n",
    "            decay_ind = int(self.leg_stac*duration)\n",
    "            envelope = np.exp(-np.arange(len(note)-decay_ind)/3000.)\n",
    "            note[decay_ind:] = np.multiply(note[decay_ind:],envelope)\n",
    "        except:\n",
    "            print('Note not fonund', note_filename)\n",
    "            note = np.zeros(duration)\n",
    "        return note[:duration]\n",
    "\n",
    "    def render_sequence(self, sequence, instrument='guitar', source_type='acoustic', preset=0, playback_speed=1, duration_scale=1, transpose=0):\n",
    "        # read from midi\n",
    "        midi_data = pretty_midi.PrettyMIDI(sequence)\n",
    "        seq,end_time = [],midi_data.get_end_time()\n",
    "        for inst in midi_data.instruments:\n",
    "            for note in inst.notes:\n",
    "                if note.start < end_time:\n",
    "                    note.velocity = self._quantize(note.velocity, self.velocities)\n",
    "                    seq.append((note.pitch, note.velocity, note.start/end_time, note.end/end_time))\n",
    "\n",
    "        total_length = int(end_time * self.sr / playback_speed)\n",
    "        data = np.zeros(total_length)\n",
    "  \n",
    "        for note, velocity, note_start, note_end in seq:\n",
    "            start_sample = int(note_start * total_length)\n",
    "            end_sample = int(note_end * total_length)\n",
    "            duration = end_sample - start_sample\n",
    "\n",
    "            duration = int(duration * duration_scale)\n",
    "            end_sample = start_sample + duration\n",
    "            \n",
    "            # Get corresponding synth file name\n",
    "            note_filename=instrument+'_'+source_type+'_'+str(preset).zfill(3)+'-'+str(note).zfill(3)+'-'+str(velocity).zfill(3)+'.wav'\n",
    "            \n",
    "            note = self._render_note(note_filename, duration, velocity)\n",
    "\n",
    "            if(end_sample <= len(data) and duration == len(note)):\n",
    "                data[start_sample:end_sample] += note\n",
    "            elif(duration > len(note) and end_sample <= len(data)):\n",
    "                data[start_sample:start_sample+len(note)] += note\n",
    "\n",
    "        data /= np.max(np.abs(data)) \n",
    "        return data, self.sr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading notes for keyboard_acoustic_000\n",
      "Notes loaded\n",
      "Instrument: \t keyboard_acoustic\n",
      "Sequence: \t D://4550//TEST//archive//albeniz//alb_esp1.mid\n",
      "Output: \t D://4550//TEST//output\\keyboard_acoustic\\alb_esp1.wav \n",
      "\n",
      "Instrument: \t keyboard_acoustic\n",
      "Sequence: \t D://4550//TEST//archive//albeniz//alb_esp2.mid\n",
      "Output: \t D://4550//TEST//output\\keyboard_acoustic\\alb_esp2.wav \n",
      "\n",
      "Preloading notes for string_acoustic_000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harrison\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\librosa\\core\\audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes loaded\n",
      "Instrument: \t string_acoustic\n",
      "Sequence: \t D://4550//TEST//archive//albeniz//alb_esp1.mid\n",
      "Output: \t D://4550//TEST//output\\string_acoustic\\alb_esp1.wav \n",
      "\n",
      "Note not fonund string_acoustic_000-097-050.wav\n",
      "Note not fonund string_acoustic_000-105-025.wav\n",
      "Note not fonund string_acoustic_000-100-025.wav\n",
      "Instrument: \t string_acoustic\n",
      "Sequence: \t D://4550//TEST//archive//albeniz//alb_esp2.mid\n",
      "Output: \t D://4550//TEST//output\\string_acoustic\\alb_esp2.wav \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instrument in instruments:\n",
    "    synth = NoteSynthesizer(\n",
    "                                dataset_path=args['nsynth_path'], \n",
    "                                sr=NSYNTH_SAMPLE_RATE, \n",
    "                                velocities=NSYNTH_VELOCITIES, \n",
    "                                transpose=float(args['transpose'])\n",
    "                            )\n",
    "    synth.preload_notes(instrument=instrument['name'], source_type=instrument['source_type'])\n",
    "\n",
    "    instrument_folder = instrument['name']+'_'+instrument['source_type']\n",
    "    if(not os.path.isdir(os.path.join(args['audios_path'], instrument_folder))): # init path\n",
    "        os.makedirs(os.path.join(args['audios_path'],instrument_folder))\n",
    "\n",
    "    for mid in midifiles:\n",
    "        _, seq_name = os.path.split(mid)\n",
    "        output_name = os.path.join(args['audios_path'], instrument_folder, os.path.splitext(seq_name)[0]+'.wav')\n",
    "        \n",
    "        print(\"Instrument: \\t\", instrument_folder)\n",
    "        print(\"Sequence: \\t\", mid)\n",
    "        print(\"Output: \\t\", output_name, '\\n')\n",
    "\n",
    "        audio, _ = synth.render_sequence(\n",
    "                                            sequence=str(mid),\n",
    "                                            instrument=instrument['name'],\n",
    "                                            source_type=instrument['source_type'],\n",
    "                                            preset=instrument['preset'],\n",
    "                                            playback_speed=float(args['playback_speed']),\n",
    "                                            duration_scale=float(args['duration_rate']),\n",
    "                                        )\n",
    "\n",
    "        if(DEFAULT_SAMPLING_RATE != NSYNTH_SAMPLE_RATE):\n",
    "            audio = librosa.core.resample(audio, NSYNTH_SAMPLE_RATE, DEFAULT_SAMPLING_RATE)\n",
    "        write_wav(output_name, DEFAULT_SAMPLING_RATE, np.array(32000.*audio, np.short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate CQT or SFTF freatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_transform(audio, nfft=1024, normalize=True, crop_hf=True):\n",
    "    window = np.hanning(nfft)\n",
    "    S = librosa.stft(audio, n_fft=nfft, hop_length=int(nfft/2), window=window)\n",
    "    mag, phase = librosa.magphase(S) #np.abs(S), np.angle(S)\n",
    "    if(crop_hf):\n",
    "        mag = remove_hf(mag)\n",
    "    if(normalize):\n",
    "        mag = 2 * mag / np.sum(window)\n",
    "    return mag, phase\n",
    "\n",
    "def forward_cqt(audio,sr):\n",
    "    cqt = librosa.cqt(audio, sr=sr,hop_length=hop_length, n_bins=8*bins_per_octave,bins_per_octave=bins_per_octave)\n",
    "    mag, phase = librosa.magphase(cqt)\n",
    "    return mag, phase\n",
    "\n",
    "def slice_magnitude(mag, slice_size):\n",
    "    magnitudes = np.stack([mag], axis=2)\n",
    "    return slice_first_dim(magnitudes, slice_size)\n",
    "\n",
    "def remove_hf(mag):\n",
    "    return mag[0:int(mag.shape[0]/2), :]\n",
    "\n",
    "def slice_first_dim(array, slice_size):\n",
    "    n_sections = int(np.floor(array.shape[1]/slice_size))\n",
    "    has_last_mag = n_sections*slice_size < array.shape[1]\n",
    "\n",
    "    last_mag = np.zeros(shape=(1, array.shape[0], slice_size, array.shape[2]))\n",
    "    last_mag[:,:,:array.shape[1]-(n_sections*slice_size),:] = array[:,n_sections*int(slice_size):,:]\n",
    "    \n",
    "    if(n_sections > 0):\n",
    "        array = np.expand_dims(array, axis=0)\n",
    "        sliced = np.split(array[:,:,0:n_sections*slice_size,:], n_sections, axis=2)\n",
    "        sliced = np.concatenate(sliced, axis=0)\n",
    "        if(has_last_mag): # Check for reminder\n",
    "            sliced = np.concatenate([sliced, last_mag], axis=0)\n",
    "    else:\n",
    "        sliced = last_mag\n",
    "    return sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard_acoustic\n",
      "alb_esp1 (33, 256, 256, 1)\n",
      "alb_esp2 (43, 256, 256, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8fb23c6ea4c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'asd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mmag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_cqt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_SAMPLING_RATE\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mmag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamplitude_to_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# amplitude to db\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mmag\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mamin\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-56f93e27d6e4>\u001b[0m in \u001b[0;36mforward_cqt\u001b[1;34m(audio, sr)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_cqt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcqt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcqt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbins_per_octave\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbins_per_octave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbins_per_octave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagphase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcqt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\librosa\\core\\constantq.py\u001b[0m in \u001b[0;36mcqt\u001b[1;34m(y, sr, hop_length, fmin, n_bins, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type)\u001b[0m\n\u001b[0;32m    261\u001b[0m             my_y = audio.resample(my_y, 2, 1,\n\u001b[0;32m    262\u001b[0m                                   \u001b[0mres_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                                   scale=True)\n\u001b[0m\u001b[0;32m    264\u001b[0m             \u001b[1;31m# The re-scale the filters to compensate for downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mfft_basis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\resampy\\core.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mresample_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args={'audios_path':'.//data//audios'\n",
    "    ,'stft_features_path':'.//data//stft_features'\n",
    "    ,'cqt_features_path':'.//data//cqt_features'\n",
    "    , 'stft':False}\n",
    "\n",
    "for instrument in os.listdir(args['audios_path']):\n",
    "    print(instrument)\n",
    "    audios_dir = args['audios_path']+'//'+instrument\n",
    "    if args['stft']:\n",
    "        features_dir = args['stft_features_path']+'//'+instrument\n",
    "    else:\n",
    "        features_dir = args['cqt_features_path']+'//'+instrument\n",
    "    \n",
    "    for file in os.listdir(audios_dir):\n",
    "        name, _ = file.split('.')\n",
    "        audio,sr = librosa.core.load(audios_dir+'//'+file, sr=44100)# get audio (return audio and sampling rate)\n",
    "        \n",
    "        if args['stft']:\n",
    "            mag, _ = forward_transform(audio)\n",
    "            mag = librosa.amplitude_to_db(mag, ref=np.min, amin=1/(2**16)) # amplitude to db\n",
    "            mag /= 20*np.log1p(1/amin ) # normalize\n",
    "            print('asd')\n",
    "        else:\n",
    "            mag, _ = forward_cqt(audio, DEFAULT_SAMPLING_RATE )\n",
    "            mag = librosa.amplitude_to_db(mag, ref=np.min, amin=1/(2**16)) # amplitude to db\n",
    "            mag /= 20*np.log1p(1/amin ) # normalize\n",
    "\n",
    "        mag_sliced = slice_magnitude(mag, mag.shape[0])\n",
    "        print(name, mag_sliced.shape)\n",
    "        \n",
    "        for i in range(mag_sliced.shape[0]):\n",
    "            out_name = features_dir+'//'+name+'_'+str(i).zfill(3)+'.npy'\n",
    "            if(not os.path.isfile(out_name)):\n",
    "                np.save(out_name, mag_sliced[i,:,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
